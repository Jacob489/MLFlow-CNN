{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd072b9f-f3c6-4047-b7c2-2a0c9bf4347f",
   "metadata": {},
   "source": [
    "# Set Up CUDA Environment\n",
    "\n",
    "This sets up environment variables needed for TensorFlow to locate CUDA libraries for GPU support. It:\n",
    "\n",
    "- Determines the conda environment path and sets it as `CUDA_PATH`.\n",
    "- On Linux, additionally sets `LD_LIBRARY_PATH` and `XLA_FLAGS` for proper CUDA library access.\n",
    "- Prints the configured CUDA path for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ccb54b-cc64-498c-bfb4-e6836595baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux environment detected - loading specialized GPU setup...\n",
      "✅ Libraries pre-loaded successfully\n",
      "✅ GPU setup complete: GPU 0 enabled and configured for training\n",
      "TensorFlow version: 2.10.1\n",
      "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# GPU and Environment Setup\n",
    "# -------------------------\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# Always silence TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Only use the gpu_setup module on Linux\n",
    "if platform.system() != \"Windows\":\n",
    "    print(\"Linux environment detected - loading specialized GPU setup...\")\n",
    "    try:\n",
    "        import gpu_setup\n",
    "    except ImportError:\n",
    "        print(\"Warning: gpu_setup.py module not found. GPU functionality may be limited.\")\n",
    "else:\n",
    "    print(\"Windows environment detected - using standard GPU configuration.\")\n",
    "\n",
    "# Import TensorFlow and print version info\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs detected:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049e8e4-cfcf-4c44-9738-f975eea81f95",
   "metadata": {},
   "source": [
    "# Import Machine Learning Libraries and Modules\n",
    "\n",
    "This cell loads the key libraries required for building and training the model. These include TensorFlow (and related modules), MLflow for tracking experiments, and additional libraries for data processing, visualization, and command-line argument parsing. Utility functions from photoz_utils and DataMakerPlus are also imported for custom data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd10b51-0127-4b61-8e70-4efc2d5fadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tensorboard\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import sys\n",
    "import getpass\n",
    "import platform\n",
    "import time\n",
    "import socket \n",
    "import subprocess\n",
    "\n",
    "# Import your local utility functions\n",
    "from photoz_utils import *\n",
    "from DataMakerPlus import *\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8eb88-4700-42c6-9211-074cb219d31f",
   "metadata": {},
   "source": [
    "# MLflow & Parameter Configuration\n",
    "\n",
    "This cell initializes your training parameters. Feel free to tweak any of these to run multiple experiments and compare how different settings affect model performance:\n",
    "\n",
    "- **image_size**: size of input images (here, 64×64)  \n",
    "- **epochs**: number of full passes through the training data (5)  \n",
    "- **batch_size**: samples per gradient update (256)  \n",
    "- **learning_rate**: step size for optimizer (0.0001)  \n",
    "- **experiment_name**: MLflow experiment under which runs will be grouped  \n",
    "- **run_name**: specific run identifier (automatically generated if `None`)  \n",
    "- **gpu_id**: index of the GPU to use (0-based)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e021af2-f818-4724-b587-85da48991911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# MLflow and Directory Setup\n",
    "# -------------------------\n",
    "params = {\n",
    "    'image_size': 64,             # Set image size to 64\n",
    "    'epochs': 200,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.0001,\n",
    "    'experiment_name': \"Galaxy_CNN_Redshift_Estimation\",\n",
    "    'run_name': None,             # Auto-generate run name if None\n",
    "    'gpu_id': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012e1da-53d1-47b1-81c1-6efd04e6fcf5",
   "metadata": {},
   "source": [
    "# MLflow Environment & Directory Setup\n",
    "\n",
    "This cell prepares your MLflow tracking, artifacts, and local experiment folders:\n",
    "\n",
    "- **Config Flags:**  \n",
    "  - `clear_tracking_store`, `clear_experiments_store`, `clear_artifacts`  \n",
    "  - **True** → delete existing data for a completely fresh run  \n",
    "  - **False** → keep prior runs/artifacts so you can compare multiple experiments\n",
    "\n",
    "- **1) Tracking Store:**  \n",
    "  - Creates `./mlruns` (and the required `.trash` subfolder).  \n",
    "  - Sets the MLflow tracking URI (platform‐aware file URI).  \n",
    "  - Selects your experiment by name, with diagnostic prints and a SQLite fallback if it fails.\n",
    "\n",
    "- **2) Artifact Staging:**  \n",
    "  - Creates (or recreates) `./MLFlowData` for staging all per-run artifacts, including:  \n",
    "    - **Training history CSVs** (`<timestamp>_training_history.csv`)  \n",
    "    - **Training curve images** (`<timestamp>_training_curves.png`)  \n",
    "    - **Prediction scatter plots** (`<timestamp>_prediction_plot.png`)\n",
    "\n",
    "- **3) Experiment Directories:**  \n",
    "  - Under `./experiments`, sets up:  \n",
    "    - `MLCheckpoints` (model weights)  \n",
    "    - `MLlogs` (TensorBoard logs)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8474b9f7-5853-4e53-86c8-bd6d47212b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/28 20:38:16 INFO mlflow.tracking.fluent: Experiment with name 'Galaxy_CNN_Redshift_Estimation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI → file:///home/jupyter-jacob/RepoCloned/MLFlow-CNN/mlruns\n",
      "Successfully set experiment to Galaxy_CNN_Redshift_Estimation\n",
      "Deleted entire MLFlowData directory.\n",
      "Local MLFlowData directory → /home/jupyter-jacob/RepoCloned/MLFlow-CNN/MLFlowData\n",
      "Deleted entire experiments directory.\n",
      "experiments/ structure set up:\n",
      "  • checkpoints → /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLCheckpoints\n",
      "  • tensorboard logs → /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLlogs\n"
     ]
    }
   ],
   "source": [
    "# ─── Config options ───────────────────────────────────────────────────────────\n",
    "clear_tracking_store    = True\n",
    "clear_experiments_store = True\n",
    "clear_artifacts         = True\n",
    "\n",
    "# ─── 1) Tracking store setup ─────────────────────────────────────────────────\n",
    "mlruns_dir = os.path.abspath(\"mlruns\")\n",
    "if clear_tracking_store and os.path.exists(mlruns_dir):\n",
    "    shutil.rmtree(mlruns_dir)\n",
    "os.makedirs(mlruns_dir, exist_ok=True)\n",
    "\n",
    "# MLflow expects a \".trash\" folder\n",
    "trash_dir = os.path.join(mlruns_dir, \".trash\")\n",
    "os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "# Set tracking URI\n",
    "if os.name == 'nt':  # Windows\n",
    "    mlruns_uri = f\"file:///{mlruns_dir.replace(os.sep, '/')}\"\n",
    "else:                # Linux/Mac\n",
    "    mlruns_uri = f\"file://{mlruns_dir}\"\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = f\"file://{mlruns_dir}\"\n",
    "print(f\"MLflow tracking URI → {mlruns_uri}\")\n",
    "mlflow.set_tracking_uri(mlruns_uri)\n",
    "\n",
    "# Set experiment\n",
    "try:\n",
    "    mlflow.set_experiment(params['experiment_name'])\n",
    "    print(f\"Successfully set experiment to {params['experiment_name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting experiment: {e}\")\n",
    "    # (Optional) diagnostics and fallback\n",
    "    print(\"MLruns exists?\", os.path.exists(mlruns_dir))\n",
    "    print(\"Trash exists?\", os.path.exists(trash_dir))\n",
    "    print(\"Contents of mlruns:\", os.listdir(mlruns_dir))\n",
    "    # Fallback to SQLite if needed\n",
    "    sqlite_uri = f\"sqlite:///{os.path.abspath('mlflow.db')}\"\n",
    "    print(\"Falling back to SQLite at\", sqlite_uri)\n",
    "    mlflow.set_tracking_uri(sqlite_uri)\n",
    "    mlflow.set_experiment(params['experiment_name'])\n",
    "    print(\"Experiment set using SQLite backend\")\n",
    "\n",
    "# ─── 2) MLFlowData (artifact staging) ────────────────────────────────────────\n",
    "mlflow_data_dir = os.path.abspath(\"MLFlowData\")\n",
    "if clear_artifacts and os.path.exists(mlflow_data_dir):\n",
    "    shutil.rmtree(mlflow_data_dir)\n",
    "    print(f\"Deleted entire MLFlowData directory.\")\n",
    "os.makedirs(mlflow_data_dir, exist_ok=True)\n",
    "print(f\"Local MLFlowData directory → {mlflow_data_dir}\")\n",
    "\n",
    "# ─── 3) experiments/ folder (checkpoints & logs) ─────────────────────────────\n",
    "base_dir = os.path.abspath(\"experiments\")\n",
    "if clear_experiments_store and os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "    print(\"Deleted entire experiments directory.\")\n",
    "checkpoint_dir = os.path.join(base_dir, \"MLCheckpoints\")\n",
    "log_dir        = os.path.join(base_dir, \"MLlogs\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(\"experiments/ structure set up:\")\n",
    "print(f\"  • checkpoints → {checkpoint_dir}\")\n",
    "print(f\"  • tensorboard logs → {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb62f52-709b-44a9-aa87-0fc013f26f6a",
   "metadata": {},
   "source": [
    "# Hyperparameters and Dataset Paths\n",
    "\n",
    "This cell extracts key training hyperparameters from the `params` dictionary and defines additional settings (like number of dense units, maximum redshift, and data format). It also stores these hyperparameters in a dictionary (`hparams`) for logging with MLflow, sets the file paths for the training, validation, and test datasets, and verifies that these dataset files exist. Adjust these paths as needed depending on where you downloaded the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb1af0-9fc8-4287-bdd6-c264e6983fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Dataset Paths and Preprocessing Setup\n",
    "# -------------------------\n",
    "import os\n",
    "\n",
    "# Check if running in CI environment\n",
    "in_ci = os.path.exists('.ci_mode') or os.environ.get('CI') or os.environ.get('GITHUB_ACTIONS') or os.environ.get('BINDER_SERVICE_HOST')\n",
    "\n",
    "# Set dataset paths based on environment\n",
    "if in_ci:\n",
    "    print(\"CI/Binder environment detected - using demo datasets\")\n",
    "    TRAIN_PATH = os.path.join(os.getcwd(), 'demo_astrodata/5x64x64_training_with_morphology.hdf5')\n",
    "    VAL_PATH = os.path.join(os.getcwd(), 'demo_astrodata/5x64x64_validation_with_morphology.hdf5')\n",
    "    TEST_PATH = os.path.join(os.getcwd(), 'demo_astrodata/5x64x64_testing_with_morphology.hdf5')\n",
    "    \n",
    "    # Extreme resource conservation for CI\n",
    "    params['epochs'] = 1  # Single epoch\n",
    "    \n",
    "    # Drastically reduce batch size to avoid memory issues\n",
    "    original_batch_size = params['batch_size']\n",
    "    params['batch_size'] = 4  # Use tiny batch size for CI\n",
    "    \n",
    "    print(f\"CI environment: Reduced epochs to {params['epochs']} and batch size from {original_batch_size} to {params['batch_size']}\")\n",
    "else:\n",
    "    print(\"Local environment detected - using original dataset paths\")\n",
    "    TRAIN_PATH = '/shared/astrodata/5x64x64_training_with_morphology.hdf5'\n",
    "    VAL_PATH = '/shared/astrodata/5x64x64_validation_with_morphology.hdf5'\n",
    "    TEST_PATH = '/shared/astrodata/5x64x64_testing_with_morphology.hdf5'\n",
    "\n",
    "# Check if files exist\n",
    "for path in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
    "\n",
    "# Rest of your original code remains unchanged\n",
    "# Prepare model checkpoint filename\n",
    "username = getpass.getuser()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, f\"{username}_cp_{timestamp}.weights.h5\")\n",
    "\n",
    "# Define generator arguments (using original preprocessing details)\n",
    "param_names = []\n",
    "for band in ['g', 'r', 'i', 'z', 'y']:\n",
    "    for col in ['cmodel_mag']:\n",
    "        param_names.append(f\"{band}_{col}\")\n",
    "\n",
    "gen_args = {\n",
    "    'image_key': 'image',\n",
    "    'numerical_keys': param_names,\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': True,             # Data scaling enabled\n",
    "    'labels_encoding': False,   # No extra label encoding\n",
    "    'batch_size': params['batch_size'],\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "train_gen = HDF5DataGenerator(TRAIN_PATH, mode='train', **gen_args)\n",
    "val_gen   = HDF5DataGenerator(VAL_PATH, mode='train', **gen_args)\n",
    "test_gen  = HDF5DataGenerator(TEST_PATH, mode='test', **gen_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f946f-7da8-4ae9-9762-03bb37cd9bdd",
   "metadata": {},
   "source": [
    "# Define the Model Architecture\n",
    "\n",
    "This cell defines the `create_model()` function, which builds a Keras model with two input branches. One branch (CNN) processes image data, and the other (NN) handles additional numerical features. The outputs from these branches are combined and passed through a final layer to produce a single prediction.\n",
    "\n",
    "The model is compiled using the Adam optimizer, a custom HSC loss function (instead of the standard mean squared error loss), and RMSE is tracked as a performance metric. This setup allows for a tailored approach to measuring prediction errors while making the architecture reusable throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7514db31-9207-49f1-9fd0-9fff5f3fccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Define inputs for image (CNN branch) and numerical data (NN branch)\n",
    "    input_cnn = Input(shape=(5, params['image_size'], params['image_size']))\n",
    "    input_nn  = Input(shape=(5,))\n",
    "    \n",
    "    # CNN branch with 7 convolutional layers and pooling\n",
    "    conv1 = Conv2D(32, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(input_cnn)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv1)\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv2)\n",
    "    conv3 = Conv2D(128, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv3)\n",
    "    conv4 = Conv2D(256, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv4)\n",
    "    conv5 = Conv2D(256, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool4)\n",
    "    pool5 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv5)\n",
    "    conv6 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first')(pool5)\n",
    "    conv7 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first')(conv6)\n",
    "    flatten = Flatten()(conv7)\n",
    "    dense1 = Dense(512, activation='tanh')(flatten)\n",
    "    dense2 = Dense(128, activation='tanh')(dense1)\n",
    "    dense3 = Dense(32, activation='tanh')(dense2)\n",
    "    \n",
    "    # NN branch: fully connected layers processing numerical inputs\n",
    "    NUM_DENSE_UNITS = 200\n",
    "    hidden1 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(input_nn)\n",
    "    hidden2 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden2)\n",
    "    hidden4 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden3)\n",
    "    hidden5 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden4)\n",
    "    hidden6 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden5)\n",
    "    \n",
    "    # Concatenate the outputs from both branches and produce the final prediction\n",
    "    concat = Concatenate()([dense3, hidden6])\n",
    "    output = Dense(1)(concat)\n",
    "    model = Model(inputs=[input_cnn, input_nn], outputs=output)\n",
    "    \n",
    "    # Define custom HSC loss function\n",
    "    def calculate_loss(y_true, y_pred):\n",
    "        dz = y_pred - y_true\n",
    "        gamma = 0.15\n",
    "        denominator = 1.0 + tf.square(dz / gamma)\n",
    "        L = 1 - 1.0 / denominator\n",
    "        return L\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "        loss=calculate_loss,\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18183c-0f34-4228-a0b4-a8eab5e66f14",
   "metadata": {},
   "source": [
    "# Define Callbacks for Logging Metrics\n",
    "\n",
    "This cell sets up several callbacks to monitor and manage training:\n",
    "\n",
    "- **TensorBoard Callback:**  \n",
    "  Logs training data for visualization in TensorBoard, including histograms of the model's layers.\n",
    "\n",
    "- **Model Checkpoint Callback:**  \n",
    "  Saves only the model weights at the end of each epoch if there is an improvement (monitored by the loss value). This ensures the best model is saved during training.\n",
    "\n",
    "- **Hyperparameter Callback:**  \n",
    "  Logs key hyperparameters (like the number of dense units, batch size, epochs, learning rate, etc.) to help track the training setup.\n",
    "\n",
    "- **MLflow Callback (Custom):**  \n",
    "  At the end of each epoch, it logs training metrics (like loss and RMSE) to MLflow for experiment tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9166704-69a1-4ce8-bfee-a4494157e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callback definitions\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "hparam_callback = hp.KerasCallback(log_dir, {\n",
    "    'num_dense_units': 200,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'num_epochs': params['epochs'],\n",
    "    'learning_rate': params['learning_rate'],\n",
    "    'z_max': 4,\n",
    "    'data_format': 'channels_first'\n",
    "})\n",
    "\n",
    "class MLflowCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for name, value in logs.items():\n",
    "            mlflow.log_metric(name, value, step=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d22cdc-a16f-40f4-9744-282bba21fb8d",
   "metadata": {},
   "source": [
    "# Training Function with MLflow Logging\n",
    "\n",
    "This cell defines the `train_model_with_mlflow()` function, which handles the entire training process and logs details using MLflow:\n",
    "\n",
    "- **Run Setup:**  \n",
    "  It sets a unique run name and logs key parameters and hyperparameters.\n",
    "\n",
    "- **Model Training:**  \n",
    "  The function creates the model, trains it using training and validation data, and uses callbacks (for TensorBoard, checkpointing, hyperparameter logging, and custom MLflow logging) during training.\n",
    "\n",
    "- **Artifact Logging:**  \n",
    "  After training, it saves the model, training history, and plots of the training loss. These artifacts are logged to MLflow.\n",
    "\n",
    "- **Prediction and Evaluation:**  \n",
    "  It generates and saves a scatter plot comparing true and predicted values, evaluates the model on test data, and logs the test metrics.\n",
    "\n",
    "- **Final Steps:**  \n",
    "  The model's summary and package requirements are saved and logged, and the function prints the MLflow Run ID to confirm completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcadf2b3-2b85-4b72-aa19-d21200d2df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Training Function with MLflow Logging\n",
    "# -------------------------\n",
    "def train_model_with_mlflow():\n",
    "    run_name = params['run_name'] or f\"GalaxyCNN_Size{params['image_size']}_Batch{params['batch_size']}_LR{params['learning_rate']}_Epochs{params['epochs']}_{username}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.set_tag(\"username\", username)\n",
    "        mlflow.log_params({**params, 'num_dense_units': 200, 'z_max': 4, 'data_format': 'channels_first'})\n",
    "        mlflow.tensorflow.autolog(\n",
    "            log_models=False,            # don’t auto‐save the model at training end\n",
    "            log_datasets=False,           \n",
    "            log_model_signatures=False,   \n",
    "            silent=True                  # suppress autolog INFO/WARNING messages\n",
    "        )\n",
    "\n",
    "        \n",
    "        model = create_model()\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=params['epochs'],\n",
    "            validation_data=val_gen,\n",
    "            callbacks=[tensorboard_callback, model_checkpoint_callback, MLflowCallback()],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        model.save(checkpoint_filepath)\n",
    "        mlflow.log_artifact(checkpoint_filepath)\n",
    "        \n",
    "        # Get unique identifiers for this run\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        file_prefix = f\"{timestamp}_{run_id[:8]}\"\n",
    "        \n",
    "        # Save training history to MLFlowData directory with timestamp\n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        history_csv_path = os.path.join(mlflow_data_dir, f\"{file_prefix}_training_history.csv\")\n",
    "        history_df.to_csv(history_csv_path, index=False)\n",
    "        mlflow.log_artifact(history_csv_path)\n",
    "        \n",
    "        # Save training curves to MLFlowData directory with timestamp\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history_df.index, history_df['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history_df.columns:\n",
    "            plt.plot(history_df.index, history_df['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Curves')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        training_curves_path = os.path.join(mlflow_data_dir, f\"{file_prefix}_training_curves.png\")\n",
    "        plt.savefig(training_curves_path)\n",
    "        mlflow.log_artifact(training_curves_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save prediction plot to MLFlowData directory with timestamp\n",
    "        predictions = model.predict(test_gen)\n",
    "        predictions = predictions.squeeze()\n",
    "        with h5py.File(TEST_PATH, 'r') as f:\n",
    "            test_labels = np.asarray(f['specz_redshift'][:])\n",
    "        test_labels = test_labels.squeeze()\n",
    "        print(\"Test labels shape:\", test_labels.shape)\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sc = plt.scatter(test_labels, predictions, c=predictions, cmap='viridis', alpha=0.7, edgecolors='w', s=50)\n",
    "        plt.plot([test_labels.min(), test_labels.max()], [test_labels.min(), test_labels.max()], 'r--', lw=2)\n",
    "        plt.xlabel(\"True Redshift\")\n",
    "        plt.ylabel(\"Predicted Redshift\")\n",
    "        plt.title(\"Prediction Scatter Plot\")\n",
    "        plt.colorbar(sc, label=\"Predicted Value\")\n",
    "        plt.tight_layout()\n",
    "        prediction_plot_path = os.path.join(mlflow_data_dir, f\"{file_prefix}_prediction_plot.png\")\n",
    "        plt.savefig(prediction_plot_path)\n",
    "        mlflow.log_artifact(prediction_plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save test metrics to MLFlowData directory with timestamp\n",
    "        test_loss, test_rmse = model.evaluate(test_gen, verbose=1)\n",
    "        mlflow.log_metric(\"test_loss\", test_loss)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        test_metrics_path = os.path.join(mlflow_data_dir, f\"{file_prefix}_test_metrics.txt\")\n",
    "        with open(test_metrics_path, \"w\") as f:\n",
    "            f.write(f\"Test Loss: {test_loss}\\nTest RMSE: {test_rmse}\\n\")\n",
    "        mlflow.log_artifact(test_metrics_path)\n",
    "        \n",
    "        mlflow.keras.log_model(model, \"model\")\n",
    "        \n",
    "        # Save model summary to MLFlowData directory with timestamp\n",
    "        model_summary_lines = []\n",
    "        model.summary(print_fn=lambda line: model_summary_lines.append(line))\n",
    "        model_summary_path = os.path.join(mlflow_data_dir, f\"{file_prefix}_model_summary.txt\")\n",
    "        with open(model_summary_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(model_summary_lines))\n",
    "        mlflow.log_artifact(model_summary_path)\n",
    "        \n",
    "        # Save requirements to MLFlowData directory with timestamp\n",
    "        import subprocess\n",
    "        requirements_path = os.path.join(mlflow_data_dir, f\"{file_prefix}_requirements.txt\")\n",
    "        subprocess.run(f\"pip freeze > {requirements_path}\", shell=True)\n",
    "        mlflow.log_artifact(requirements_path)\n",
    "        \n",
    "        print(f\"Training complete. MLflow Run ID: {mlflow.active_run().info.run_id}\")\n",
    "        print(f\"Artifacts saved to MLFlowData with prefix: {file_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362db322-a934-461a-9fc0-4a3b7d471aad",
   "metadata": {},
   "source": [
    "# Run Training\n",
    "\n",
    "This final cell calls the `train_model_with_mlflow()` function, which starts the training process, logs experiment details with MLflow, saves model checkpoints, and evaluates the model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df1b1e1-0bff-4a13-8a39-896510bb949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.3590 - root_mean_squared_error: 0.5704\n",
      "Epoch 1: loss improved from inf to 0.35870, saving model to /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLCheckpoints/jupyter-jacob_cp_2025-04-28_20-38-34.weights.h5\n",
      "800/800 [==============================] - 30s 34ms/step - loss: 0.3587 - root_mean_squared_error: 0.5704 - val_loss: 0.2897 - val_root_mean_squared_error: 0.5518\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.2844 - root_mean_squared_error: 0.5759\n",
      "Epoch 2: loss improved from 0.35870 to 0.28438, saving model to /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLCheckpoints/jupyter-jacob_cp_2025-04-28_20-38-34.weights.h5\n",
      "800/800 [==============================] - 26s 33ms/step - loss: 0.2844 - root_mean_squared_error: 0.5759 - val_loss: 0.2629 - val_root_mean_squared_error: 0.5555\n",
      "Epoch 3/5\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.2589 - root_mean_squared_error: 0.5111\n",
      "Epoch 3: loss improved from 0.28438 to 0.25873, saving model to /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLCheckpoints/jupyter-jacob_cp_2025-04-28_20-38-34.weights.h5\n",
      "800/800 [==============================] - 24s 30ms/step - loss: 0.2587 - root_mean_squared_error: 0.5110 - val_loss: 0.2656 - val_root_mean_squared_error: 0.4563\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.2238 - root_mean_squared_error: 0.4412\n",
      "Epoch 4: loss improved from 0.25873 to 0.22379, saving model to /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLCheckpoints/jupyter-jacob_cp_2025-04-28_20-38-34.weights.h5\n",
      "800/800 [==============================] - 23s 28ms/step - loss: 0.2238 - root_mean_squared_error: 0.4412 - val_loss: 0.2009 - val_root_mean_squared_error: 0.4348\n",
      "Epoch 5/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.2062 - root_mean_squared_error: 0.4331\n",
      "Epoch 5: loss improved from 0.22379 to 0.20610, saving model to /home/jupyter-jacob/RepoCloned/MLFlow-CNN/experiments/MLCheckpoints/jupyter-jacob_cp_2025-04-28_20-38-34.weights.h5\n",
      "800/800 [==============================] - 24s 30ms/step - loss: 0.2061 - root_mean_squared_error: 0.4330 - val_loss: 0.1897 - val_root_mean_squared_error: 0.4373\n",
      "160/160 [==============================] - 4s 24ms/step\n",
      "Test labels shape: (40914,)\n",
      "Predictions shape: (40914,)\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.0000e+00 - root_mean_squared_error: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/28 20:41:00 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2025/04/28 20:41:05 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp4ogp6efi/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/04/28 20:41:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. MLflow Run ID: 9e955422f6fa4c85b0ca62b3d9a20f7d\n",
      "Artifacts saved to MLFlowData with prefix: 20250428_204051_9e955422\n"
     ]
    }
   ],
   "source": [
    "# Simply run this cell to start training the model, log metrics, and save artifacts via MLflow.\n",
    "\n",
    "train_model_with_mlflow()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa9c07-4697-4cb3-82c1-dd750f74c43c",
   "metadata": {},
   "source": [
    "## MLflow UI Startup Helper\n",
    "\n",
    "This cell automates launching the MLflow User Interface (UI) so you can easily visualize your experiment tracking results.\n",
    "\n",
    "**What it Does:**\n",
    "\n",
    "1.  **Cleans Up:** Stops any old MLflow UI processes that might still be running.\n",
    "2.  **Configures:** Sets the `MLFLOW_TRACKING_URI` environment variable to point to the local `./mlruns` directory, telling MLflow where your experiment data is stored.\n",
    "3.  **Launches UI:** Starts the MLflow UI server as a background process. It listens on port `5000` and is configured to be accessible from other machines on the network (host `0.0.0.0`).\n",
    "4.  **Provides Access Links:** After a short pause (to allow the server to start), it prints several URLs you can use to access the UI:\n",
    "    * **Local/SSH Tunnel:** `http://localhost:5000`\n",
    "    * **Direct Network:** `http://<hostname>:5000` (uses your machine's actual hostname)\n",
    "    * **Jupyter Proxy:** A common URL format for accessing services through Jupyter.\n",
    "    * It also gives you the `ssh` command needed to create a tunnel from your local machine for secure access if needed.\n",
    "\n",
    "**How to Use:**\n",
    "\n",
    "* Run this cell.\n",
    "* Click one of the generated URLs (usually the `localhost:5000` link if using SSH tunneling or working locally, or the direct/proxy link otherwise) to open the MLflow dashboard in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3c4133-1f28-4526-a9cf-1b98fed1550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Local (SSH‐tunnel):  http://localhost:5000\n",
      "→ Direct (if open):   http://altair:5000\n",
      "→ Jupyter proxy:       http://altair:8888/proxy/5000/\n",
      "\n",
      "To tunnel from your laptop, run:\n",
      "  ssh -N -L 5000:localhost:5000 your_user@altair\n"
     ]
    }
   ],
   "source": [
    "# 1) Kill any old MLflow UI\n",
    "subprocess.run(\"pkill -f 'mlflow ui' || true\", shell=True)\n",
    "\n",
    "# 2) Point the CLI at your new ./mlruns folder\n",
    "mlruns_dir = os.path.abspath(\"mlruns\")\n",
    "os.environ['MLFLOW_TRACKING_URI'] = f\"file://{mlruns_dir}\"\n",
    "\n",
    "# 3) Fire up the UI on all interfaces:5000\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"mlflow\", \"ui\",\n",
    "    \"--host\", \"0.0.0.0\", \"--port\", \"5000\",\n",
    "    \"--backend-store-uri\", os.environ['MLFLOW_TRACKING_URI']\n",
    "]\n",
    "subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# 4) Wait a sec\n",
    "time.sleep(2)\n",
    "\n",
    "# 5) Build and print clickable URLs\n",
    "host = socket.gethostname()\n",
    "print(f\"→ Local (SSH‐tunnel):  http://localhost:5000\")\n",
    "print(f\"→ Direct (if open):   http://{host}:5000\")\n",
    "print(f\"→ Jupyter proxy:       http://{host}:8888/proxy/5000/\")\n",
    "print(\"\\nTo tunnel from your laptop, run:\")\n",
    "print(f\"  ssh -N -L 5000:localhost:5000 your_user@{host}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843a2e7-244c-43ee-9c90-9c85a03e7e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UCLA)",
   "language": "python",
   "name": "ucla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
