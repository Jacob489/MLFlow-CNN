{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd072b9f-f3c6-4047-b7c2-2a0c9bf4347f",
   "metadata": {},
   "source": [
    "# Set Up CUDA Environment\n",
    "\n",
    "This sets up environment variables needed for TensorFlow to locate CUDA libraries for GPU support. It:\n",
    "\n",
    "- Determines the conda environment path and sets it as `CUDA_PATH`.\n",
    "- On Linux, additionally sets `LD_LIBRARY_PATH` and `XLA_FLAGS` for proper CUDA library access.\n",
    "- Prints the configured CUDA path for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c792d9-507b-444f-bb71-15aa6486c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set CUDA_PATH to: c:\\Users\\truen\\miniconda3\\envs\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment and CUDA Configuration\n",
    "# This cell sets up the environment variables and prints the CUDA path.\n",
    "import os\n",
    "import sys\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import platform\n",
    "\n",
    "# Get conda environment path and set CUDA_PATH (works for both Linux and Windows)\n",
    "conda_env_path = os.path.dirname(os.path.dirname(sys.executable))\n",
    "os.environ['CUDA_PATH'] = conda_env_path\n",
    "print(f\"Set CUDA_PATH to: {conda_env_path}\")\n",
    "\n",
    "# Setup for Linux systems\n",
    "if platform.system() != \"Windows\":\n",
    "    cuda_path = os.path.join(conda_env_path, \"lib\")\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{cuda_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "    os.environ['XLA_FLAGS'] = f\"--xla_gpu_cuda_data_dir={conda_env_path}\"\n",
    "    print(f\"Set LD_LIBRARY_PATH and XLA_FLAGS for Linux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049e8e4-cfcf-4c44-9738-f975eea81f95",
   "metadata": {},
   "source": [
    "# Import Machine Learning Libraries and Modules\n",
    "\n",
    "This cell loads the key libraries required for building and training the model. These include TensorFlow (and related modules), MLflow for tracking experiments, and additional libraries for data processing, visualization, and command-line argument parsing. Utility functions from photoz_utils and DataMakerPlus are also imported for custom data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd10b51-0127-4b61-8e70-4efc2d5fadb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tensorboard\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import os\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# Import your local utility functions\n",
    "from photoz_utils import *\n",
    "from DataMakerPlus import *\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b36891-b450-4e65-880c-75ca01d7a1d6",
   "metadata": {},
   "source": [
    "# Configure GPU Settings\n",
    "This cell checks for available GPUs and enables memory growth. Enabling memory growth ensures that TensorFlow only allocates as much GPU memory as needed, rather than grabbing all available memory. This is especially useful when running multiple experiments on the same machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac717455-312c-4bb3-a4fc-5cf0dfcaf1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled for GPU 0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# GPU Setup: List GPUs and enable memory growth\n",
    "# -------------------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Memory growth enabled for GPU {i}\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "else:\n",
    "    print(\"No GPUs found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8eb88-4700-42c6-9211-074cb219d31f",
   "metadata": {},
   "source": [
    "# Hyperparameters and GPU Configuration\n",
    "\n",
    "This cell defines a dictionary of key training parameters (e.g., image size, epochs, batch size, learning rate, experiment name) for interactive use. It then checks for multiple GPUs and, if available, sets the specified GPU for training. Finally, it confirms whether a GPU is available or if the training will proceed on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e021af2-f818-4724-b587-85da48991911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used for training\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# MLflow and Directory Setup\n",
    "# -------------------------\n",
    "params = {\n",
    "    'image_size': 64,             # Set image size to 64\n",
    "    'epochs': 200,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.0001,\n",
    "    'experiment_name': \"Galaxy_CNN_Redshift_Estimation\",\n",
    "    'run_name': None,             # Auto-generate run name if None\n",
    "    'gpu_id': 0\n",
    "}\n",
    "\n",
    "if len(gpus) > 1:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(params['gpu_id'])\n",
    "    print(f\"Using GPU {params['gpu_id']}\")\n",
    "\n",
    "if gpus:\n",
    "    with tf.device('/GPU:0'):\n",
    "        print(\"GPU is available and will be used for training\")\n",
    "else:\n",
    "    print(\"Proceeding with CPU.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012e1da-53d1-47b1-81c1-6efd04e6fcf5",
   "metadata": {},
   "source": [
    "### MLflow Environment Reset\n",
    "\n",
    "- **Cleanup Flags:**  \n",
    "  Three flags (`clear_tracking_store`, `clear_experiments_store`, and `clear_artifacts`) decide if existing runs, directories, or artifact files should be removed.  \n",
    "  * Set them each to \"True\" if you wish to start a new run from scratch and remove previous runs, or set them to \"False\" if you intend to do multiple runs.\n",
    "\n",
    "- **MLflow Setup:**  \n",
    "  It creates the `mlruns` directory, deletes any leftover `.trash` folder, sets the tracking URI, and initializes the experiment using a provided name.  \n",
    "  * **Note:** To change where MLflow stores its run data, modify the `mlruns_dir` variable (e.g., `mlruns_dir = os.path.abspath(\"my_custom_mlruns_dir\")`).\n",
    "\n",
    "- **Experiment Directories:**  \n",
    "  It ensures directories for checkpoints and logs exist, or removes them if a full reset is requested.  \n",
    "  * **Note:** To change where experiment artifacts are stored, update the `base_dir` variable (e.g., `base_dir = os.path.abspath(\"my_custom_experiments_dir\")`).\n",
    "\n",
    "- **Artifact Removal:**  \n",
    "  Optionally, it deletes specific artifact files (e.g., logs, plots, metrics) for a clean slate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8474b9f7-5853-4e53-86c8-bd6d47212b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted .trash directory.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid parent directory 'C:\\Users\\truen\\MLFlow-CNN\\mlruns\\.trash'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexperiment_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# If any issue occurs, attempt to remove .trash and try again\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[1;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\tracking\\client.py:1701\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m \n\u001b[0;32m   1672\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;124;03m    Lifecycle_stage: active\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:586\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m    name: The experiment name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m    :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:355\u001b[0m, in \u001b[0;36mFileStore.get_experiment_by_name\u001b[1;34m(self, experiment_name)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_experiments(\n\u001b[0;32m    349\u001b[0m         view_type\u001b[38;5;241m=\u001b[39mViewType\u001b[38;5;241m.\u001b[39mALL,\n\u001b[0;32m    350\u001b[0m         max_results\u001b[38;5;241m=\u001b[39mnumber_to_get,\n\u001b[0;32m    351\u001b[0m         filter_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    352\u001b[0m         page_token\u001b[38;5;241m=\u001b[39mnext_page_token,\n\u001b[0;32m    353\u001b[0m     )\n\u001b[1;32m--> 355\u001b[0m experiments \u001b[38;5;241m=\u001b[39m \u001b[43mget_results_from_paginated_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaginated_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpagination_wrapper_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEARCH_MAX_RESULTS_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m experiments[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(experiments) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\utils\\__init__.py:237\u001b[0m, in \u001b[0;36mget_results_from_paginated_fn\u001b[1;34m(paginated_fn, max_results_per_page, max_results)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     page_results \u001b[38;5;241m=\u001b[39m \u001b[43mpaginated_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results_per_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_page_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m all_results\u001b[38;5;241m.\u001b[39mextend(page_results)\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:348\u001b[0m, in \u001b[0;36mFileStore.get_experiment_by_name.<locals>.pagination_wrapper_func\u001b[1;34m(number_to_get, next_page_token)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpagination_wrapper_func\u001b[39m(number_to_get, next_page_token):\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mview_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mViewType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_to_get\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_page_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:323\u001b[0m, in \u001b[0;36mFileStore.search_experiments\u001b[1;34m(self, view_type, max_results, filter_string, order_by, page_token)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m view_type \u001b[38;5;241m==\u001b[39m ViewType\u001b[38;5;241m.\u001b[39mDELETED_ONLY \u001b[38;5;129;01mor\u001b[39;00m view_type \u001b[38;5;241m==\u001b[39m ViewType\u001b[38;5;241m.\u001b[39mALL:\n\u001b[1;32m--> 323\u001b[0m     experiment_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_deleted_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m experiments \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:295\u001b[0m, in \u001b[0;36mFileStore._get_deleted_experiments\u001b[1;34m(self, full_path)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_deleted_experiments\u001b[39m(\u001b[38;5;28mself\u001b[39m, full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_subdirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrash_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\utils\\file_utils.py:162\u001b[0m, in \u001b[0;36mlist_subdirs\u001b[1;34m(dir_name, full_path)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03mEquivalent to UNIX command:\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m  ``find $dir_name -depth 1 -type d``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    list of all directories directly under 'dir_name'.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\utils\\file_utils.py:145\u001b[0m, in \u001b[0;36mlist_all\u001b[1;34m(root, filter_func, full_path)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(root):\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parent directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m matches \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root) \u001b[38;5;28;01mif\u001b[39;00m filter_func(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, x))]\n",
      "\u001b[1;31mException\u001b[0m: Invalid parent directory 'C:\\Users\\truen\\MLFlow-CNN\\mlruns\\.trash'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to delete .trash on retry:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexperiment_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clear_experiments_store \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(base_dir):\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[1;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m         experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\tracking\\client.py:1701\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m \n\u001b[0;32m   1672\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;124;03m        Lifecycle_stage: active\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:586\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:355\u001b[0m, in \u001b[0;36mFileStore.get_experiment_by_name\u001b[1;34m(self, experiment_name)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpagination_wrapper_func\u001b[39m(number_to_get, next_page_token):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_experiments(\n\u001b[0;32m    349\u001b[0m         view_type\u001b[38;5;241m=\u001b[39mViewType\u001b[38;5;241m.\u001b[39mALL,\n\u001b[0;32m    350\u001b[0m         max_results\u001b[38;5;241m=\u001b[39mnumber_to_get,\n\u001b[0;32m    351\u001b[0m         filter_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    352\u001b[0m         page_token\u001b[38;5;241m=\u001b[39mnext_page_token,\n\u001b[0;32m    353\u001b[0m     )\n\u001b[1;32m--> 355\u001b[0m experiments \u001b[38;5;241m=\u001b[39m \u001b[43mget_results_from_paginated_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaginated_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpagination_wrapper_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEARCH_MAX_RESULTS_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m experiments[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(experiments) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\utils\\__init__.py:237\u001b[0m, in \u001b[0;36mget_results_from_paginated_fn\u001b[1;34m(paginated_fn, max_results_per_page, max_results)\u001b[0m\n\u001b[0;32m    235\u001b[0m     page_results \u001b[38;5;241m=\u001b[39m paginated_fn(num_to_get, next_page_token)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     page_results \u001b[38;5;241m=\u001b[39m \u001b[43mpaginated_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results_per_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_page_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m all_results\u001b[38;5;241m.\u001b[39mextend(page_results)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(page_results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m page_results\u001b[38;5;241m.\u001b[39mtoken:\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:348\u001b[0m, in \u001b[0;36mFileStore.get_experiment_by_name.<locals>.pagination_wrapper_func\u001b[1;34m(number_to_get, next_page_token)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpagination_wrapper_func\u001b[39m(number_to_get, next_page_token):\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mview_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mViewType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_to_get\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_page_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:323\u001b[0m, in \u001b[0;36mFileStore.search_experiments\u001b[1;34m(self, view_type, max_results, filter_string, order_by, page_token)\u001b[0m\n\u001b[0;32m    321\u001b[0m     experiment_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_active_experiments(full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m view_type \u001b[38;5;241m==\u001b[39m ViewType\u001b[38;5;241m.\u001b[39mDELETED_ONLY \u001b[38;5;129;01mor\u001b[39;00m view_type \u001b[38;5;241m==\u001b[39m ViewType\u001b[38;5;241m.\u001b[39mALL:\n\u001b[1;32m--> 323\u001b[0m     experiment_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_deleted_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m experiments \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp_id \u001b[38;5;129;01min\u001b[39;00m experiment_ids:\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:295\u001b[0m, in \u001b[0;36mFileStore._get_deleted_experiments\u001b[1;34m(self, full_path)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_deleted_experiments\u001b[39m(\u001b[38;5;28mself\u001b[39m, full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_subdirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrash_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\utils\\file_utils.py:162\u001b[0m, in \u001b[0;36mlist_subdirs\u001b[1;34m(dir_name, full_path)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_subdirs\u001b[39m(dir_name, full_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    Equivalent to UNIX command:\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m      ``find $dir_name -depth 1 -type d``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m        list of all directories directly under 'dir_name'.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\truen\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\utils\\file_utils.py:145\u001b[0m, in \u001b[0;36mlist_all\u001b[1;34m(root, filter_func, full_path)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"List all entities directly under 'dir_name' that satisfy 'filter_func'\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(root):\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parent directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m matches \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root) \u001b[38;5;28;01mif\u001b[39;00m filter_func(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, x))]\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches] \u001b[38;5;28;01mif\u001b[39;00m full_path \u001b[38;5;28;01melse\u001b[39;00m matches\n",
      "\u001b[1;31mException\u001b[0m: Invalid parent directory 'C:\\Users\\truen\\MLFlow-CNN\\mlruns\\.trash'"
     ]
    }
   ],
   "source": [
    "# Optionally clear previous MLflow runs and experiment artifacts\n",
    "clear_tracking_store = False     \n",
    "clear_experiments_store = False      \n",
    "clear_artifacts = False             \n",
    "\n",
    "mlruns_dir = os.path.abspath(\"mlruns\").replace(\"\\\\\", \"/\")\n",
    "# If clear_tracking_store is False, we keep existing data, but still remove .trash automatically.\n",
    "os.makedirs(mlruns_dir, exist_ok=True)\n",
    "trash_path = os.path.join(mlruns_dir, \".trash\")\n",
    "if os.path.exists(trash_path):\n",
    "    try:\n",
    "        shutil.rmtree(trash_path)\n",
    "        print(\"Deleted .trash directory.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error deleting .trash directory:\", e)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_dir}\")\n",
    "try:\n",
    "    mlflow.set_experiment(params['experiment_name'])\n",
    "except Exception as e:\n",
    "    # If any issue occurs, attempt to remove .trash and try again\n",
    "    if os.path.exists(trash_path):\n",
    "        try:\n",
    "            shutil.rmtree(trash_path)\n",
    "            print(\"Force-deleted .trash directory on retry.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to delete .trash on retry:\", e)\n",
    "    mlflow.set_experiment(params['experiment_name'])\n",
    "\n",
    "base_dir = os.path.abspath(\"experiments\")\n",
    "if clear_experiments_store and os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "    print(\"Deleted entire experiments directory.\")\n",
    "checkpoint_dir = os.path.join(base_dir, \"MLCheckpoints\")\n",
    "log_dir = os.path.join(base_dir, \"MLlogs\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(\"Experiments directory is set up.\")\n",
    "\n",
    "if clear_artifacts:\n",
    "    artifact_files = [\n",
    "        \"training_history.csv\",\n",
    "        \"training_curves.png\",\n",
    "        \"prediction_plot.png\",\n",
    "        \"test_metrics.txt\",\n",
    "        \"model_summary.txt\",\n",
    "        \"requirements.txt\"\n",
    "    ]\n",
    "    for fname in artifact_files:\n",
    "        if os.path.exists(fname):\n",
    "            os.remove(fname)\n",
    "            print(f\"Deleted previous {fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb62f52-709b-44a9-aa87-0fc013f26f6a",
   "metadata": {},
   "source": [
    "# Hyperparameters and Dataset Paths\n",
    "\n",
    "This cell extracts key training hyperparameters from the `params` dictionary and defines additional settings (like number of dense units, maximum redshift, and data format). It also stores these hyperparameters in a dictionary (`hparams`) for logging with MLflow, sets the file paths for the training, validation, and test datasets, and verifies that these dataset files exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92eb1af0-9fc8-4287-bdd6-c264e6983fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Dataset Paths and Preprocessing Setup\n",
    "# -------------------------\n",
    "TRAIN_PATH = r'E:\\Datasets\\5x64x64_training_with_morphology.hdf5'\n",
    "VAL_PATH   = r'E:\\Datasets\\5x64x64_validation_with_morphology.hdf5'\n",
    "TEST_PATH  = r'E:\\Datasets\\5x64x64_testing_with_morphology.hdf5'\n",
    "\n",
    "for path in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
    "\n",
    "# Prepare model checkpoint filename\n",
    "username = getpass.getuser()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, f\"{username}_cp_{timestamp}.weights.h5\")\n",
    "\n",
    "# Define generator arguments (using original preprocessing details)\n",
    "param_names = []\n",
    "for band in ['g', 'r', 'i', 'z', 'y']:\n",
    "    for col in ['cmodel_mag']:\n",
    "        param_names.append(f\"{band}_{col}\")\n",
    "\n",
    "gen_args = {\n",
    "    'image_key': 'image',\n",
    "    'numerical_keys': param_names,\n",
    "    'y_key': 'specz_redshift',\n",
    "    'scaler': True,             # Data scaling enabled\n",
    "    'labels_encoding': False,   # No extra label encoding\n",
    "    'batch_size': params['batch_size'],\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "train_gen = HDF5DataGenerator(TRAIN_PATH, mode='train', **gen_args)\n",
    "val_gen   = HDF5DataGenerator(VAL_PATH, mode='train', **gen_args)\n",
    "test_gen  = HDF5DataGenerator(TEST_PATH, mode='test', **gen_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f946f-7da8-4ae9-9762-03bb37cd9bdd",
   "metadata": {},
   "source": [
    "# Define the Model Architecture\n",
    "\n",
    "This cell defines the `create_model()` function, which builds a Keras model with two input branches. One branch (CNN) processes image data, and the other (NN) handles additional numerical features. The outputs from these branches are combined and passed through a final layer to produce a single prediction.\n",
    "\n",
    "The model is compiled using the Adam optimizer, a custom HSC loss function (instead of the standard mean squared error loss), and RMSE is tracked as a performance metric. This setup allows for a tailored approach to measuring prediction errors while making the architecture reusable throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7514db31-9207-49f1-9fd0-9fff5f3fccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Model Definition: Full CNN Architecture with Custom HSC Loss\n",
    "# -------------------------\n",
    "def create_model():\n",
    "    # Define inputs for image (CNN branch) and numerical data (NN branch)\n",
    "    input_cnn = Input(shape=(5, params['image_size'], params['image_size']))\n",
    "    input_nn  = Input(shape=(5,))\n",
    "    \n",
    "    # CNN branch with 7 convolutional layers and pooling\n",
    "    conv1 = Conv2D(32, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(input_cnn)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv1)\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv2)\n",
    "    conv3 = Conv2D(128, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv3)\n",
    "    conv4 = Conv2D(256, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv4)\n",
    "    conv5 = Conv2D(256, kernel_size=(3, 3), activation='tanh', padding='same', data_format='channels_first')(pool4)\n",
    "    pool5 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(conv5)\n",
    "    conv6 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first')(pool5)\n",
    "    conv7 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', data_format='channels_first')(conv6)\n",
    "    flatten = Flatten()(conv7)\n",
    "    dense1 = Dense(512, activation='tanh')(flatten)\n",
    "    dense2 = Dense(128, activation='tanh')(dense1)\n",
    "    dense3 = Dense(32, activation='tanh')(dense2)\n",
    "    \n",
    "    # NN branch: fully connected layers processing numerical inputs\n",
    "    NUM_DENSE_UNITS = 200\n",
    "    hidden1 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(input_nn)\n",
    "    hidden2 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden2)\n",
    "    hidden4 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden3)\n",
    "    hidden5 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden4)\n",
    "    hidden6 = Dense(NUM_DENSE_UNITS, activation=\"relu\")(hidden5)\n",
    "    \n",
    "    # Concatenate the outputs from both branches and produce the final prediction\n",
    "    concat = Concatenate()([dense3, hidden6])\n",
    "    output = Dense(1)(concat)\n",
    "    model = Model(inputs=[input_cnn, input_nn], outputs=output)\n",
    "    \n",
    "    # Define custom HSC loss function\n",
    "    def calculate_loss(z_photo, z_spec):\n",
    "        dz = z_photo - z_spec\n",
    "        gamma = 0.15\n",
    "        denominator = 1.0 + tf.square(dz / gamma)\n",
    "        L = 1 - 1.0 / denominator\n",
    "        return L\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "                  loss=calculate_loss,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18183c-0f34-4228-a0b4-a8eab5e66f14",
   "metadata": {},
   "source": [
    "# Define Callbacks for Logging Metrics\n",
    "\n",
    "This cell sets up several callbacks to monitor and manage training:\n",
    "\n",
    "- **TensorBoard Callback:**  \n",
    "  Logs training data for visualization in TensorBoard, including histograms of the model's layers.\n",
    "\n",
    "- **Model Checkpoint Callback:**  \n",
    "  Saves only the model weights at the end of each epoch if there is an improvement (monitored by the loss value). This ensures the best model is saved during training.\n",
    "\n",
    "- **Hyperparameter Callback:**  \n",
    "  Logs key hyperparameters (like the number of dense units, batch size, epochs, learning rate, etc.) to help track the training setup.\n",
    "\n",
    "- **MLflow Callback (Custom):**  \n",
    "  At the end of each epoch, it logs training metrics (like loss and RMSE) to MLflow for experiment tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9166704-69a1-4ce8-bfee-a4494157e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    "    verbose=True)\n",
    "hparam_callback = hp.KerasCallback(log_dir, {\n",
    "    'num_dense_units': 200,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'num_epochs': params['epochs'],\n",
    "    'learning_rate': params['learning_rate'],\n",
    "    'z_max': 4,\n",
    "    'data_format': 'channels_first'\n",
    "})\n",
    "\n",
    "class MLflowCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for name, value in logs.items():\n",
    "            mlflow.log_metric(name, value, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d22cdc-a16f-40f4-9744-282bba21fb8d",
   "metadata": {},
   "source": [
    "# Training Function with MLflow Logging\n",
    "\n",
    "This cell defines the `train_model_with_mlflow()` function, which handles the entire training process and logs details using MLflow:\n",
    "\n",
    "- **Run Setup:**  \n",
    "  It sets a unique run name and logs key parameters and hyperparameters.\n",
    "\n",
    "- **Model Training:**  \n",
    "  The function creates the model, trains it using training and validation data, and uses callbacks (for TensorBoard, checkpointing, hyperparameter logging, and custom MLflow logging) during training.\n",
    "\n",
    "- **Artifact Logging:**  \n",
    "  After training, it saves the model, training history, and plots of the training loss. These artifacts are logged to MLflow.\n",
    "\n",
    "- **Prediction and Evaluation:**  \n",
    "  It generates and saves a scatter plot comparing true and predicted values, evaluates the model on test data, and logs the test metrics.\n",
    "\n",
    "- **Final Steps:**  \n",
    "  The model's summary and package requirements are saved and logged, and the function prints the MLflow Run ID to confirm completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadf2b3-2b85-4b72-aa19-d21200d2df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Training Function with MLflow Logging\n",
    "# -------------------------\n",
    "def train_model_with_mlflow():\n",
    "    run_name = params['run_name'] or f\"GalaxyCNN_Size{params['image_size']}_Batch{params['batch_size']}_LR{params['learning_rate']}_Epochs{params['epochs']}_{username}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.set_tag(\"username\", username)\n",
    "        mlflow.log_params({**params, 'num_dense_units': 200, 'z_max': 4, 'data_format': 'channels_first'})\n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        model = create_model()\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=params['epochs'],\n",
    "            validation_data=val_gen,\n",
    "            callbacks=[tensorboard_callback, model_checkpoint_callback, hparam_callback, MLflowCallback()],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        model.save(checkpoint_filepath)\n",
    "        mlflow.log_artifact(checkpoint_filepath)\n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        history_csv = \"training_history.csv\"\n",
    "        history_df.to_csv(history_csv, index=False)\n",
    "        mlflow.log_artifact(history_csv)\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history_df.index, history_df['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history_df.columns:\n",
    "            plt.plot(history_df.index, history_df['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Curves')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        training_curves_path = \"training_curves.png\"\n",
    "        plt.savefig(training_curves_path)\n",
    "        mlflow.log_artifact(training_curves_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # ---- Prediction Plot: Visualization with Colormap ----\n",
    "        predictions = model.predict(test_gen)\n",
    "        predictions = predictions.squeeze()\n",
    "        with h5py.File(TEST_PATH, 'r') as f:\n",
    "            test_labels = np.asarray(f['specz_redshift'][:])\n",
    "        test_labels = test_labels.squeeze()\n",
    "        print(\"Test labels shape:\", test_labels.shape)\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Color points by the predicted value using the 'viridis' colormap\n",
    "        sc = plt.scatter(test_labels, predictions, c=predictions, cmap='viridis', alpha=0.7, edgecolors='w', s=50)\n",
    "        plt.plot([test_labels.min(), test_labels.max()], [test_labels.min(), test_labels.max()], 'r--', lw=2)\n",
    "        plt.xlabel(\"True Redshift\")\n",
    "        plt.ylabel(\"Predicted Redshift\")\n",
    "        plt.title(\"Prediction Scatter Plot\")\n",
    "        plt.colorbar(sc, label=\"Predicted Value\")\n",
    "        plt.tight_layout()\n",
    "        prediction_plot_path = \"prediction_plot.png\"\n",
    "        plt.savefig(prediction_plot_path)\n",
    "        mlflow.log_artifact(prediction_plot_path)\n",
    "        plt.close()\n",
    "        # -----------------------------------------------------------\n",
    "                        \n",
    "        test_loss, test_rmse = model.evaluate(test_gen, verbose=1)\n",
    "        mlflow.log_metric(\"test_loss\", test_loss)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        with open(\"test_metrics.txt\", \"w\") as f:\n",
    "            f.write(f\"Test Loss: {test_loss}\\nTest RMSE: {test_rmse}\\n\")\n",
    "        mlflow.log_artifact(\"test_metrics.txt\")\n",
    "        \n",
    "        mlflow.keras.log_model(model, \"model\")\n",
    "        \n",
    "        model_summary_lines = []\n",
    "        model.summary(print_fn=lambda line: model_summary_lines.append(line))\n",
    "        summary_path = \"model_summary.txt\"\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(model_summary_lines))\n",
    "        mlflow.log_artifact(summary_path)\n",
    "        \n",
    "        import subprocess\n",
    "        subprocess.run(\"pip freeze > requirements.txt\", shell=True)\n",
    "        mlflow.log_artifact(\"requirements.txt\")\n",
    "        \n",
    "        print(f\"Training complete. MLflow Run ID: {mlflow.active_run().info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362db322-a934-461a-9fc0-4a3b7d471aad",
   "metadata": {},
   "source": [
    "# Run Training\n",
    "\n",
    "This final cell calls the `train_model_with_mlflow()` function, which starts the training process, logs experiment details with MLflow, saves model checkpoints, and evaluates the model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f00714c-5217-4e6e-af2b-d065930b91ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/06 18:43:54 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'DataMakerPlus.HDF5DataGenerator'>. Dataset logging skipped.\n",
      "2025/04/06 18:43:54 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'DataMakerPlus.HDF5DataGenerator'>. Dataset logging skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.4357 - root_mean_squared_error: 0.7095\n",
      "Epoch 1: loss improved from inf to 0.43571, saving model to c:\\Users\\truen\\MLFlow-CNN\\experiments\\MLCheckpoints\\truen_cp_2025-04-06_18-43-39.weights.h5\n",
      "800/800 [==============================] - 66s 77ms/step - loss: 0.4357 - root_mean_squared_error: 0.7095 - val_loss: 0.2968 - val_root_mean_squared_error: 0.5551\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.2975 - root_mean_squared_error: 0.5818\n",
      "Epoch 2: loss improved from 0.43571 to 0.29747, saving model to c:\\Users\\truen\\MLFlow-CNN\\experiments\\MLCheckpoints\\truen_cp_2025-04-06_18-43-39.weights.h5\n",
      "800/800 [==============================] - 51s 63ms/step - loss: 0.2975 - root_mean_squared_error: 0.5818 - val_loss: 0.3414 - val_root_mean_squared_error: 0.5617\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.2721 - root_mean_squared_error: 0.5540\n",
      "Epoch 3: loss improved from 0.29747 to 0.27213, saving model to c:\\Users\\truen\\MLFlow-CNN\\experiments\\MLCheckpoints\\truen_cp_2025-04-06_18-43-39.weights.h5\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.2721 - root_mean_squared_error: 0.5540 - val_loss: 0.2289 - val_root_mean_squared_error: 0.4807\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.2378 - root_mean_squared_error: 0.4967\n",
      "Epoch 4: loss improved from 0.27213 to 0.23782, saving model to c:\\Users\\truen\\MLFlow-CNN\\experiments\\MLCheckpoints\\truen_cp_2025-04-06_18-43-39.weights.h5\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.2378 - root_mean_squared_error: 0.4967 - val_loss: 0.2136 - val_root_mean_squared_error: 0.4558\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.2157 - root_mean_squared_error: 0.4850\n",
      "Epoch 5: loss improved from 0.23782 to 0.21573, saving model to c:\\Users\\truen\\MLFlow-CNN\\experiments\\MLCheckpoints\\truen_cp_2025-04-06_18-43-39.weights.h5\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.2157 - root_mean_squared_error: 0.4850 - val_loss: 0.2115 - val_root_mean_squared_error: 0.4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/06 18:48:25 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2025/04/06 18:48:25 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\truen\\AppData\\Local\\Temp\\tmpufbfche_\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\truen\\AppData\\Local\\Temp\\tmpufbfche_\\model\\data\\model\\assets\n",
      "2025/04/06 18:48:37 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\truen\\AppData\\Local\\Temp\\tmpufbfche_\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/04/06 18:48:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 11s 66ms/step\n",
      "Test labels shape: (40914,)\n",
      "Predictions shape: (40914,)\n",
      "160/160 [==============================] - 4s 26ms/step - loss: 0.0000e+00 - root_mean_squared_error: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/06 18:48:55 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\truen\\AppData\\Local\\Temp\\tmpoys3p6vn\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\truen\\AppData\\Local\\Temp\\tmpoys3p6vn\\model\\data\\model\\assets\n",
      "2025/04/06 18:49:04 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\truen\\AppData\\Local\\Temp\\tmpoys3p6vn\\model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/04/06 18:49:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. MLflow Run ID: 89364fa1bfc94d31ae95373dc76dbade\n"
     ]
    }
   ],
   "source": [
    "# Simply run this cell to start training the model, log metrics, and save artifacts via MLflow.\n",
    "\n",
    "train_model_with_mlflow()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
